{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9c65b16c"},"source":["# Task\n","Create all the necessary cells to download the \"wlasl-processed\" dataset from Kaggle, process the videos to extract keypoints using MediaPipe, load and prepare the data, define, compile, and train an LSTM model using TensorFlow/Keras, convert the trained model to TensorFlow Lite format, test the TFLite model, generate an action list for Android, and provide Android integration guidance. Ensure the video processing step processes all available instances."]},{"cell_type":"markdown","source":["Install the necessary dependencies"],"metadata":{"id":"XqlT_y4jc1c1"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install kaggle mediapipe tensorflow opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"G6bIQcNLc4yq","executionInfo":{"status":"ok","timestamp":1756393796113,"user_tz":-330,"elapsed":39813,"user":{"displayName":"Bala Kumaran","userId":"01169282059865830559"}},"outputId":"1df0aca8-e7bf-4c8d-c812-e11119e8463d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n","Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n","Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n","Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n","Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n","Collecting numpy<2 (from mediapipe)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n","Collecting protobuf (from kaggle)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-python\n","  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.3)\n","INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-contrib-python (from mediapipe)\n","  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n","Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-python, opencv-contrib-python, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.12.0.88\n","    Uninstalling opencv-python-4.12.0.88:\n","      Successfully uninstalled opencv-python-4.12.0.88\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.12.0.88\n","    Uninstalling opencv-contrib-python-4.12.0.88:\n","      Successfully uninstalled opencv-contrib-python-4.12.0.88\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 opencv-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cv2","google","numpy"]},"id":"72a996623ca94742ba9f8edabb3c4fb8"}},"metadata":{}}]},{"cell_type":"markdown","source":[],"metadata":{"id":"QMjumQ3Ec5xM"}},{"cell_type":"markdown","metadata":{"id":"32e00719"},"source":["## Mount google drive and set up project directory\n","\n","### Subtask:\n","Mount Google Drive to save processed data and models persistently and define the project root directory within Drive.\n"]},{"cell_type":"markdown","metadata":{"id":"83840dc0"},"source":["**Reasoning**:\n","Mount Google Drive, define the project root and subdirectories within Drive, and create these directories to ensure persistent storage for downloaded data and extracted keypoints. This directly addresses all steps of the current subtask.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed190a61","executionInfo":{"status":"ok","timestamp":1756394123657,"user_tz":-330,"elapsed":11398,"user":{"displayName":"Bala Kumaran","userId":"01169282059865830559"}},"outputId":"0a6435a0-ccd8-4877-c827-3e99c85f1812"},"source":["from google.colab import drive\n","import os\n","\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully.\")\n","\n","# Define base directory in your Google Drive for this project\n","# You can change 'my_sign_language_project' to whatever you prefer\n","GOOGLE_DRIVE_PROJECT_ROOT = '/content/drive/My Drive/my_sign_language_project'\n","os.makedirs(GOOGLE_DRIVE_PROJECT_ROOT, exist_ok=True)\n","print(f\"Project directory created in Google Drive: {GOOGLE_DRIVE_PROJECT_ROOT}\")\n","\n","# Define paths for dataset and keypoints within the project directory\n","WLASL_DATA_ROOT_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'wlasl_data')\n","OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'wlasl_keypoints')\n","\n","os.makedirs(WLASL_DATA_ROOT_DRIVE, exist_ok=True)\n","os.makedirs(OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE, exist_ok=True)\n","\n","print(f\"WLASL data will be downloaded to: {WLASL_DATA_ROOT_DRIVE}\")\n","print(f\"Processed keypoints will be saved to: {OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE}\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully.\n","Project directory created in Google Drive: /content/drive/My Drive/my_sign_language_project\n","WLASL data will be downloaded to: /content/drive/My Drive/my_sign_language_project/wlasl_data\n","Processed keypoints will be saved to: /content/drive/My Drive/my_sign_language_project/wlasl_keypoints\n"]}]},{"cell_type":"markdown","metadata":{"id":"78c8174e"},"source":["## Set up Kaggle API\n","\n","### Subtask:\n","Copy the `kaggle.json` file to the correct directory and set permissions to enable downloading datasets from Kaggle."]},{"cell_type":"markdown","metadata":{"id":"d3c8a755"},"source":["**Reasoning**:\n","Copy the uploaded `kaggle.json` file to the `.kaggle` directory in the user's home directory and set the appropriate file permissions. This is necessary for the Kaggle API to authenticate and download datasets."]},{"cell_type":"code","metadata":{"id":"b81fc09f"},"source":["import os\n","import shutil\n","\n","kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n","kaggle_json_path = os.path.join(\"/content\", \"kaggle.json\")\n","\n","# Create the .kaggle directory if it doesn't exist\n","if not os.path.exists(kaggle_dir):\n","    os.makedirs(kaggle_dir)\n","    print(f\"Created directory: {kaggle_dir}\")\n","\n","# Copy the kaggle.json file into the .kaggle directory\n","if os.path.exists(kaggle_json_path):\n","    shutil.copy(kaggle_json_path, kaggle_dir)\n","    print(f\"Copied {kaggle_json_path} to {kaggle_dir}\")\n","    # Set permissions - Kaggle sometimes requires this\n","    os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\n","    print(\"Set permissions for kaggle.json\")\n","else:\n","    print(f\"Error: {kaggle_json_path} not found. Please ensure you have uploaded kaggle.json to /content.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2806e2d"},"source":["## Download WLASL Dataset to Google Drive\n","\n","### Subtask:\n","Download the wlasl-processed dataset from Kaggle directly to the designated Google Drive folder."]},{"cell_type":"markdown","metadata":{"id":"7f83524e"},"source":["**Reasoning**:\n","Use the Kaggle API command to download and unzip the specified dataset (`risangbaskoro/wlasl-processed`) directly into the `wlasl_data` directory within the Google Drive project root. This ensures the dataset is stored persistently in Google Drive for later processing."]},{"cell_type":"code","metadata":{"id":"21c18e88"},"source":["# ==============================================================================\n","# Download the wlasl-processed dataset from Kaggle to Google Drive\n","# Ensure you have mounted your Google Drive and set up GOOGLE_DRIVE_PROJECT_ROOT\n","# ==============================================================================\n","import os\n","\n","# Define the dataset slug\n","DATASET_SLUG = \"risangbaskoro/wlasl-processed\"\n","\n","# Define the target directory in Google Drive (set in the previous cell)\n","# Make sure GOOGLE_DRIVE_PROJECT_ROOT is defined and Drive is mounted\n","# GOOGLE_DRIVE_PROJECT_ROOT = '/content/drive/My Drive/my_sign_language_project' # Ensure this is defined\n","# Assuming GOOGLE_DRIVE_PROJECT_ROOT is defined from the Google Drive mounting cell (ed190a61)\n","try:\n","    _ = GOOGLE_DRIVE_PROJECT_ROOT\n","except NameError:\n","    print(\"Error: GOOGLE_DRIVE_PROJECT_ROOT is not defined.\")\n","    print(\"Please ensure the Google Drive mounting cell (ed190a61) was run successfully.\")\n","    exit()\n","\n","\n","WLASL_DATA_ROOT_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'wlasl_data')\n","\n","print(f\"Downloading {DATASET_SLUG} dataset from Kaggle to Google Drive...\")\n","\n","# Use the Kaggle API command to download and unzip to the Drive path\n","# Note: You need to have your kaggle.json correctly set up (as done in previous steps - cell b81fc09f)\n","!kaggle datasets download -d {DATASET_SLUG} -p \"{WLASL_DATA_ROOT_DRIVE}\" --unzip\n","\n","print(\"\\nDataset downloaded successfully to Google Drive.\")\n","print(f\"Data is located at: {WLASL_DATA_ROOT_DRIVE}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6c25f9ca"},"source":["## Process WLASL Videos and Extract Keypoints, then Load and Prepare Data\n","\n","### Subtask:\n","Process the downloaded video files from Google Drive to extract keypoints using MediaPipe, saving the results to Google Drive. This step will process all available instances by removing the processing limit. Afterward, load the extracted keypoint data, pad sequences, split into training and testing sets, and one-hot encode the labels."]},{"cell_type":"markdown","metadata":{"id":"aa3c474c"},"source":["**Reasoning**:\n","Combine the video processing and data loading/preparation steps into a single cell to ensure that the data loading occurs only after the video processing (with the processing limit removed) has completed, providing sufficient data for training. This addresses the previous issues with insufficient data for train/test split."]},{"cell_type":"code","metadata":{"id":"611e4c01"},"source":["# ==============================================================================\n","# Step 1.5: Process WLASL Videos and Extract Keypoints\n","# This code processes the downloaded WLASL video files to extract keypoints.\n","# ==============================================================================\n","import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","import json\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","\n","print(\"Starting WLASL video processing and keypoint extraction...\")\n","\n","# --- Configuration for the downloaded dataset (using Google Drive paths) ---\n","# Ensure GOOGLE_DRIVE_PROJECT_ROOT is defined and Drive is mounted (run cell ed190a61 first)\n","try:\n","    _ = GOOGLE_DRIVE_PROJECT_ROOT\n","except NameError:\n","    print(\"Error: GOOGLE_DRIVE_PROJECT_ROOT is not defined.\")\n","    print(\"Please ensure the Google Drive mounting cell (ed190a61) was run successfully.\")\n","    exit()\n","\n","WLASL_DATA_ROOT_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'wlasl_data')\n","\n","WLASL_JSON_FILE = os.path.join(WLASL_DATA_ROOT_DRIVE, 'WLASL_v0.3.json') # Read JSON from Drive\n","WLASL_VIDEO_ROOT = os.path.join(WLASL_DATA_ROOT_DRIVE, 'videos') # Read videos from Drive\n","\n","# --- Output configuration for extracted keypoints (using Google Drive path) ---\n","OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'wlasl_keypoints')\n","os.makedirs(OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE, exist_ok=True)\n","OUTPUT_WLASL_LANDMARKS_ROOT = OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE # Use the Drive path for the rest of the code\n","\n","\n","# Initialize Mediapipe Holistic model\n","# Re-initialize holistic to ensure it's ready after potential interruptions\n","mp_holistic = mp.solutions.holistic\n","holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n","\n","def extract_keypoints(results):\n","    \"\"\"Extracts keypoints from MediaPipe results.\"\"\"\n","    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n","    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n","    # Include face landmarks if desired (add 468*3 zeros if results.face_landmarks is None)\n","    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n","    # return np.concatenate([pose, lh, rh, face])\n","    return np.concatenate([pose, lh, rh])\n","\n","\n","# Load the JSON file containing video information and labels\n","try:\n","    with open(WLASL_JSON_FILE, 'r') as f:\n","        wlasl_metadata = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: {WLASL_JSON_FILE} not found. Ensure the dataset was downloaded correctly to Google Drive and the JSON file exists at: {WLASL_JSON_FILE}\")\n","    exit()\n","\n","processed_videos_count = 0\n","skipped_videos_count = 0\n","# processing_limit = 1 # Setting a limit for testing purposes - remove or increase for full dataset\n","# Removing or increasing the limit to process all data\n","# processing_limit = 21083 # Set to the total number of instances or comment out\n","# To process ALL data, we will remove the limit check in the loop\n","\n","# Flatten the list of instances from the WLASL_v0.3.json structure\n","all_instances_with_gloss = []\n","for entry in wlasl_metadata:\n","    gloss = entry['gloss']\n","    if 'instances' in entry and isinstance(entry['instances'], list):\n","        for instance in entry['instances']:\n","            if all(key in instance for key in ['video_id', 'frame_start', 'frame_end']):\n","                 all_instances_with_gloss.append({'video_id': instance['video_id'], 'gloss': gloss, 'instance': instance})\n","\n","print(f\"Found {len(all_instances_with_gloss)} video instances with required info in the selected JSON ({os.path.basename(WLASL_JSON_FILE)}).\")\n","# Print that the limit is removed\n","print(\"Processing all available instances (processing_limit removed).\")\n","\n","\n","# Iterate through all instances\n","for item in all_instances_with_gloss:\n","    # if processed_videos_count >= processing_limit: # Remove this limit check\n","    #     print(f\"Processing limit of {processing_limit} reached. Stopping video processing.\")\n","    #     break\n","\n","    gloss = item['gloss']\n","    instance = item['instance']\n","    video_id = item['video_id']\n","\n","    video_filename = f'{video_id}.mp4'\n","    video_path = os.path.join(WLASL_VIDEO_ROOT, video_filename)\n","\n","    output_gloss_path = os.path.join(OUTPUT_WLASL_LANDMARKS_ROOT, gloss)\n","    os.makedirs(output_gloss_path, exist_ok=True)\n","\n","    instance_id = instance.get('instance_id', f\"{instance['frame_start']}_{instance['frame_end']}\")\n","    output_npy_filename = f'{video_id}_{instance_id}.npy'\n","    output_npy_path = os.path.join(output_gloss_path, output_npy_filename)\n","\n","    # Skip processing if the output file already exists (for saving progress)\n","    if os.path.exists(output_npy_path):\n","        skipped_videos_count += 1\n","        processed_videos_count += 1 # Count already processed files towards the limit\n","        continue\n","\n","    # Check if the video file exists\n","    if not os.path.exists(video_path):\n","        skipped_videos_count += 1\n","        continue\n","\n","    cap = cv2.VideoCapture(video_path)\n","    sequence_data = []\n","    frame_count = 0\n","\n","    start_frame = instance['frame_start']\n","    end_frame = instance['frame_end']\n","\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if end_frame == -1 or end_frame > total_frames:\n","        end_frame = total_frames\n","\n","    if start_frame < 0 or start_frame >= end_frame:\n","        cap.release()\n","        skipped_videos_count += 1\n","        continue\n","\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","    current_frame_pos = start_frame\n","\n","    while cap.isOpened() and current_frame_pos < end_frame:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        try:\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        except cv2.error as e:\n","            current_frame_pos = int(cap.get(cv2.CAP_PROP_POS_FRAMES)) + 1\n","            continue\n","\n","        results = holistic.process(image)\n","        keypoints = extract_keypoints(results)\n","        sequence_data.append(keypoints)\n","\n","        frame_count += 1\n","        current_frame_pos = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n","\n","    cap.release()\n","\n","    if sequence_data and len(sequence_data) > 0:\n","        np.save(output_npy_path, np.array(sequence_data))\n","        processed_videos_count += 1\n","    else:\n","        skipped_videos_count += 1\n","\n","    if (processed_videos_count + skipped_videos_count) % 10 == 0:\n","        print(f\"  Progress: Attempted {processed_videos_count + skipped_videos_count} instances. Processed {processed_videos_count}, Skipped {skipped_videos_count}.\")\n","\n","\n","print(\"\\nWLASL video processing and keypoint extraction complete.\")\n","print(f\"Successfully processed {processed_videos_count} video instances.\")\n","print(f\"Skipped or failed to process {skipped_videos_count} video instances.\")\n","\n","# ==============================================================================\n","# Step 5: Load and Prepare WLASL Processed Data (re-run after processing more videos)\n","# ==============================================================================\n","print(\"\\nLoading and preparing WLASL processed data...\")\n","\n","# --- Configuration for the downloaded dataset ---\n","# OUTPUT_WLASL_LANDMARKS_ROOT is already set to the Drive path from the processing step above\n","WLASL_JSON_FILE = os.path.join(WLASL_DATA_ROOT_DRIVE, 'WLASL_v0.3.json') # Use the Drive path for JSON\n","\n","\n","# Initialize empty lists to store sequences and labels.\n","sequences = []\n","labels = []\n","\n","# Load the JSON metadata to create the gloss-to-label mapping\n","try:\n","    with open(WLASL_JSON_FILE, 'r') as f:\n","        wlasl_metadata = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: {WLASL_JSON_FILE} not found. Please ensure the dataset was downloaded correctly.\")\n","    exit()\n","\n","action_to_label = {}\n","actions_list = []\n","for entry in wlasl_metadata:\n","    gloss = entry['gloss']\n","    if gloss not in action_to_label:\n","        action_to_label[gloss] = len(action_to_label)\n","        actions_list.append(gloss)\n","\n","print(f\"Created action to label mapping with {len(actions_list)} unique actions.\")\n","\n","# Iterate through the gloss directories within the keypoints root directory and load .npy files\n","loaded_count = 0\n","skipped_count = 0\n","\n","print(\"\\nAttempting to load keypoints from processed video files...\")\n","\n","# Check if the keypoints directory exists\n","if not os.path.exists(OUTPUT_WLASL_LANDMARKS_ROOT):\n","    print(f\"Error: Keypoints directory not found: {OUTPUT_WLASL_LANDMARKS_ROOT}\")\n","    print(\"Please ensure the video processing step above ran successfully to generate the keypoint files.\")\n","else:\n","    for gloss_dir in os.listdir(OUTPUT_WLASL_LANDMARKS_ROOT):\n","        gloss_path = os.path.join(OUTPUT_WLASL_LANDMARKS_ROOT, gloss_dir)\n","\n","        if os.path.isdir(gloss_path) and gloss_dir in action_to_label:\n","            label = action_to_label[gloss_dir]\n","            for npy_file in os.listdir(gloss_path):\n","                if npy_file.endswith('.npy'):\n","                    npy_file_path = os.path.join(gloss_path, npy_file)\n","                    try:\n","                        data = np.load(npy_file_path)\n","                        if data.shape[0] > 0:\n","                             sequences.append(data)\n","                             labels.append(label)\n","                             loaded_count += 1\n","                        else:\n","                             skipped_count += 1\n","                    except Exception as e:\n","                        print(f\"Error loading {npy_file_path}: {e}\")\n","                        skipped_count += 1\n","\n","print(f(\"\\nFinished loading data.\"))\n","print(f(\"Successfully loaded {loaded_count} sequences.\"))\n","print(f(\"Skipped {skipped_count} sequences (empty or failed to load).\"))\n","print(f(\"Number of loaded sequences: {len(sequences)}\"))\n","print(f(\"Number of corresponding labels: {len(labels)}\"))\n","\n","if len(sequences) == 0:\n","    print(\"No sequences were loaded. Cannot proceed with training data preparation.\")\n","elif len(sequences) < 2:\n","     print(\"Loaded only one sequence. Need at least two sequences with different labels to split into training and testing sets for classification.\")\n","elif len(set(labels)) < 2:\n","     print(\"Loaded sequences for only one unique action. Need sequences for at least two different actions to split into training and testing sets for classification.\")\n","else:\n","    # Determine a maximum sequence length for padding.\n","    try:\n","        max_sequence_length = max(seq.shape[0] for seq in sequences)\n","        print(f\"Maximum sequence length found: {max_sequence_length}\")\n","    except ValueError:\n","        print(\"Error: Could not determine maximum sequence length from loaded sequences.\")\n","        max_sequence_length = 0\n","\n","    if max_sequence_length > 0:\n","        if len(sequences[0].shape) > 1:\n","            n_features = sequences[0].shape[1]\n","        else:\n","             print(\"Warning: Sequence data is 1D. Assuming number of features per frame is the size of the 1D array.\")\n","             n_features = sequences[0].shape[0]\n","\n","        print(f\"Number of features per frame: {n_features}\")\n","        print(f\"Padding sequences to length: {max_sequence_length}\")\n","        print(f\"Expected padded sequences shape: ({len(sequences)}, {max_sequence_length}, {n_features})\")\n","\n","\n","        padded_sequences = np.zeros((len(sequences), max_sequence_length, n_features))\n","\n","        for i, sequence in enumerate(sequences):\n","            if sequence.shape[0] > max_sequence_length:\n","                padded_sequences[i, :, :] = sequence[:max_sequence_length, :]\n","            else:\n","                padded_sequences[i, :sequence.shape[0], :] = sequence\n","\n","        print(f\"Padded sequences shape: {padded_sequences.shape}\")\n","\n","        labels = np.array(labels)\n","        print(f\"Labels array shape: {labels.shape}\")\n","\n","        # Split the data into training and testing sets.\n","        try:\n","            X_train, X_test, y_train, y_test = train_test_split(\n","                padded_sequences,\n","                labels,\n","                test_size=0.2,\n","                random_state=42,\n","                stratify=labels\n","            )\n","\n","            print(f\"\\nTraining data shape (X_train): {X_train.shape}\")\n","            print(f\"Testing data shape (X_test): {X_test.shape}\")\n","            print(f\"Training labels shape (y_train): {y_train.shape}\")\n","            print(f\"Testing labels shape (y_test): {y_test.shape}\")\n","\n","            # Convert the integer labels in the training and testing sets to one-hot encoded format.\n","            num_classes = len(actions_list)\n","            y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n","            y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n","\n","            print(f\"\\nOne-hot encoded training labels shape (y_train_one_hot): {y_train_one_hot.shape}\")\n","            print(f\"One-hot encoded testing labels shape (y_test_one_hot): {y_test_one_hot.shape}\")\n","\n","            actions = actions_list\n","\n","            print(\"\\nData loading, padding, splitting, and one-hot encoding complete.\")\n","\n","        except ValueError as e:\n","            print(f\"Error during train/test split or one-hot encoding: {e}\")\n","            print(\"This likely means there isn't enough data or not enough unique classes loaded for splitting.\")\n","    else:\n","        print(\"Cannot proceed with padding and splitting as maximum sequence length is 0.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4c4c007f"},"source":["## Build and train a model\n","\n","### Subtask:\n","Define and train a suitable neural network model (e.g., an LSTM or a simple feed-forward network) using the preprocessed keypoint data."]},{"cell_type":"code","source":["import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","import json\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# ==============================================================================\n","# Step 5: Load and Prepare WLASL Processed Data (re-run after processing more videos)\n","# ==============================================================================\n","print(\"\\nLoading and preparing WLASL processed data...\")\n","\n","\n","WLASL_DATA_ROOT_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'wlasl_data')\n","\n","WLASL_JSON_FILE = os.path.join(WLASL_DATA_ROOT_DRIVE, 'WLASL_v0.3.json') # Read JSON from Drive\n","WLASL_VIDEO_ROOT = os.path.join(WLASL_DATA_ROOT_DRIVE, 'videos') # Read videos from Drive\n","\n","# --- Output configuration for extracted keypoints (using Google Drive path) ---\n","OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'wlasl_keypoints')\n","os.makedirs(OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE, exist_ok=True)\n","OUTPUT_WLASL_LANDMARKS_ROOT = OUTPUT_WLASL_LANDMARKS_ROOT_DRIVE # Use the Drive path for the rest of the code\n","\n","# --- Configuration for the downloaded dataset ---\n","# OUTPUT_WLASL_LANDMARKS_ROOT is already set to the Drive path from the processing step above\n","WLASL_JSON_FILE = os.path.join(WLASL_DATA_ROOT_DRIVE, 'WLASL_v0.3.json') # Use the Drive path for JSON\n","\n","\n","# Initialize empty lists to store sequences and labels.\n","sequences = []\n","labels = []\n","\n","# Load the JSON metadata to create the gloss-to-label mapping\n","try:\n","    with open(WLASL_JSON_FILE, 'r') as f:\n","        wlasl_metadata = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: {WLASL_JSON_FILE} not found. Please ensure the dataset was downloaded correctly.\")\n","    exit()\n","\n","action_to_label = {}\n","actions_list = []\n","for entry in wlasl_metadata:\n","    gloss = entry['gloss']\n","    if gloss not in action_to_label:\n","        action_to_label[gloss] = len(action_to_label)\n","        actions_list.append(gloss)\n","\n","print(f\"Created action to label mapping with {len(actions_list)} unique actions.\")\n","\n","# Iterate through the gloss directories within the keypoints root directory and load .npy files\n","loaded_count = 0\n","skipped_count = 0\n","\n","print(\"\\nAttempting to load keypoints from processed video files...\")\n","\n","# Check if the keypoints directory exists\n","if not os.path.exists(OUTPUT_WLASL_LANDMARKS_ROOT):\n","    print(f\"Error: Keypoints directory not found: {OUTPUT_WLASL_LANDMARKS_ROOT}\")\n","    print(\"Please ensure the video processing step above ran successfully to generate the keypoint files.\")\n","else:\n","    for gloss_dir in os.listdir(OUTPUT_WLASL_LANDMARKS_ROOT):\n","        gloss_path = os.path.join(OUTPUT_WLASL_LANDMARKS_ROOT, gloss_dir)\n","\n","        if os.path.isdir(gloss_path) and gloss_dir in action_to_label:\n","            label = action_to_label[gloss_dir]\n","            for npy_file in os.listdir(gloss_path):\n","                if npy_file.endswith('.npy'):\n","                    npy_file_path = os.path.join(gloss_path, npy_file)\n","                    try:\n","                        data = np.load(npy_file_path, allow_pickle=True) # Added allow_pickle=True here\n","                        if data.shape[0] > 0:\n","                             sequences.append(data)\n","                             labels.append(label)\n","                             loaded_count += 1\n","                        else:\n","                             skipped_count += 1\n","                    except Exception as e:\n","                        print(f\"Error loading {npy_file_path}: {e}\")\n","                        skipped_count += 1\n","\n","print(\"\\nFinished loading data.\")\n","# print(f(\"Successfully loaded {loaded_count} sequences.\"))\n","# print(f(\"Skipped {skipped_count} sequences (empty or failed to load).\"))\n","# print(f(\"Number of loaded sequences: {len(sequences)}\"))\n","# print(f(\"Number of corresponding labels: {len(labels)}\"))\n","\n","if len(sequences) == 0:\n","    print(\"No sequences were loaded. Cannot proceed with training data preparation.\")\n","elif len(sequences) < 2:\n","     print(\"Loaded only one sequence. Need at least two sequences with different labels to split into training and testing sets for classification.\")\n","elif len(set(labels)) < 2:\n","     print(\"Loaded sequences for only one unique action. Need sequences for at least two different actions to split into training and testing sets for classification.\")\n","else:\n","    # Determine a maximum sequence length for padding.\n","    try:\n","        max_sequence_length = max(seq.shape[0] for seq in sequences)\n","        print(f\"Maximum sequence length found: {max_sequence_length}\")\n","    except ValueError:\n","        print(\"Error: Could not determine maximum sequence length from loaded sequences.\")\n","        max_sequence_length = 0\n","\n","    if max_sequence_length > 0:\n","        if len(sequences[0].shape) > 1:\n","            n_features = sequences[0].shape[1]\n","        else:\n","             print(\"Warning: Sequence data is 1D. Assuming number of features per frame is the size of the 1D array.\")\n","             n_features = sequences[0].shape[0]\n","\n","        print(f\"Number of features per frame: {n_features}\")\n","        print(f\"Padding sequences to length: {max_sequence_length}\")\n","        print(f\"Expected padded sequences shape: ({len(sequences)}, {max_sequence_length}, {n_features})\")\n","\n","\n","        padded_sequences = np.zeros((len(sequences), max_sequence_length, n_features))\n","\n","        for i, sequence in enumerate(sequences):\n","            if sequence.shape[0] > max_sequence_length:\n","                padded_sequences[i, :, :] = sequence[:max_sequence_length, :]\n","            else:\n","                padded_sequences[i, :sequence.shape[0], :] = sequence\n","\n","        print(f\"Padded sequences shape: {padded_sequences.shape}\")\n","\n","        labels = np.array(labels)\n","        print(f\"Labels array shape: {labels.shape}\")\n","\n","        # Split the data into training and testing sets.\n","        try:\n","            X_train, X_test, y_train, y_test = train_test_split(\n","                padded_sequences,\n","                labels,\n","                test_size=0.2,\n","                random_state=42\n","                # Removed stratify=labels because of insufficient samples per class\n","            )\n","\n","            print(f\"\\nTraining data shape (X_train): {X_train.shape}\")\n","            print(f\"Testing data shape (X_test): {X_test.shape}\")\n","            print(f\"Training labels shape (y_train): {y_train.shape}\")\n","            print(f\"Testing labels shape (y_test): {y_test.shape}\")\n","\n","            # Convert the integer labels in the training and testing sets to one-hot encoded format.\n","            num_classes = len(actions_list)\n","            y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n","            y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n","\n","            print(f\"\\nOne-hot encoded training labels shape (y_train_one_hot): {y_train_one_hot.shape}\")\n","            print(f\"One-hot encoded testing labels shape (y_test_one_hot): {y_test_one_hot.shape}\")\n","\n","            actions = actions_list\n","\n","            print(\"\\nData loading, padding, splitting, and one-hot encoding complete.\")\n","\n","        except ValueError as e:\n","            print(f\"Error during train/test split or one-hot encoding: {e}\")\n","            print(\"This likely means there isn't enough data or not enough unique classes loaded for splitting.\")\n","    else:\n","        print(\"Cannot proceed with padding and splitting as maximum sequence length is 0.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpGjlOqZEUzV","executionInfo":{"status":"ok","timestamp":1756395655215,"user_tz":-330,"elapsed":1197423,"user":{"displayName":"Bala Kumaran","userId":"01169282059865830559"}},"outputId":"f2e2ac7e-e5c8-4ca3-80a7-ab7998711951"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading and preparing WLASL processed data...\n","Created action to label mapping with 2000 unique actions.\n","\n","Attempting to load keypoints from processed video files...\n","Error loading /content/drive/My Drive/my_sign_language_project/wlasl_keypoints/book/07068_17.npy: Failed to interpret file '/content/drive/My Drive/my_sign_language_project/wlasl_keypoints/book/07068_17.npy' as a pickle\n","\n","Finished loading data.\n","Maximum sequence length found: 194\n","Number of features per frame: 225\n","Padding sequences to length: 194\n","Expected padded sequences shape: (5084, 194, 225)\n","Padded sequences shape: (5084, 194, 225)\n","Labels array shape: (5084,)\n","\n","Training data shape (X_train): (4067, 194, 225)\n","Testing data shape (X_test): (1017, 194, 225)\n","Training labels shape (y_train): (4067,)\n","Testing labels shape (y_test): (1017,)\n","\n","One-hot encoded training labels shape (y_train_one_hot): (4067, 2000)\n","One-hot encoded testing labels shape (y_test_one_hot): (1017, 2000)\n","\n","Data loading, padding, splitting, and one-hot encoding complete.\n"]}]},{"cell_type":"markdown","metadata":{"id":"1c1e7d01"},"source":["**Reasoning**:\n","Define the LSTM model architecture using TensorFlow/Keras, including LSTM, Dropout, and Dense layers, based on the input shape and number of classes determined during data preparation."]},{"cell_type":"code","metadata":{"id":"aa8cb2ee","executionInfo":{"status":"ok","timestamp":1756395655665,"user_tz":-330,"elapsed":406,"user":{"displayName":"Bala Kumaran","userId":"01169282059865830559"}},"colab":{"base_uri":"https://localhost:8080/","height":524},"outputId":"85cf1c72-66f2-4cbd-8729-517016ecaf5c"},"source":["# ==============================================================================\n","# Step 2: Define the Model Architecture\n","# ==============================================================================\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","print(\"Defining the model architecture...\")\n","\n","# Get the number of features per frame and the maximum sequence length from the preprocessed data\n","# Ensure X_train, max_sequence_length, and num_classes are available from the previous data loading step (cell 611e4c01)\n","try:\n","    input_shape = (max_sequence_length, n_features) # Use variables from cell 611e4c01\n","    output_units = num_classes # Use variable from cell 611e4c01\n","    print(f\"Input shape for the model: {input_shape}\")\n","    print(f\"Output units (number of classes): {output_units}\")\n","\n","except NameError:\n","    print(\"Error: Required variables (max_sequence_length, n_features, num_classes) from data loading step (cell 611e4c01) not found.\")\n","    print(\"Please ensure cell 611e4c01 was run successfully before defining the model.\")\n","    # Exit or handle the error appropriately if variables are missing\n","    exit()\n","\n","\n","model = Sequential()\n","model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n","model.add(Dropout(0.2))\n","model.add(LSTM(128, return_sequences=True, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(LSTM(64, return_sequences=False, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(output_units, activation='softmax')) # Output layer with softmax for classification\n","\n","print(\"Model architecture defined.\")\n","\n","# Display the model summary\n","model.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Defining the model architecture...\n","Input shape for the model: (194, 225)\n","Output units (number of classes): 2000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Model architecture defined.\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m74,240\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m98,816\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2000\u001b[0m)           │       \u001b[38;5;34m130,000\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">74,240</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2000</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">130,000</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m356,624\u001b[0m (1.36 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,624</span> (1.36 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,624\u001b[0m (1.36 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,624</span> (1.36 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"6f53db42"},"source":["## Compile and Train the Model\n","\n","### Subtask:\n","Compile the defined model with a suitable optimizer and loss function, and train it using the prepared training and validation data."]},{"cell_type":"markdown","metadata":{"id":"4ac5c921"},"source":["**Reasoning**:\n","Compile and train the defined Keras model using the prepared training and testing data, employing an Adam optimizer, categorical crossentropy loss, and categorical accuracy as the metric. An Early Stopping callback is included to monitor validation loss and prevent overfitting."]},{"cell_type":"code","metadata":{"id":"f889c2de","executionInfo":{"status":"ok","timestamp":1756399075581,"user_tz":-330,"elapsed":1068420,"user":{"displayName":"Bala Kumaran","userId":"01169282059865830559"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"24ad38b5-9eb2-42c6-bd9e-8ca39ad1e19c"},"source":["# ==============================================================================\n","# Step 2.5: Compile and Train the Model\n","# ==============================================================================\n","print(\"Compiling and training the model with RMSprop optimizer and adjusted architecture...\")\n","\n","# Ensure X_train, X_test, y_train_one_hot, y_test_one_hot are available from data loading (cell 611e4c01)\n","try:\n","    _ = X_train.shape\n","    _ = y_train_one_hot.shape\n","    _ = X_test.shape\n","    _ = y_test_one_hot.shape\n","    _ = model # Ensure model was defined in cell aa8cb2ee\n","except NameError:\n","    print(\"Error: Required variables (X_train, y_train_one_hot, X_test, y_test_one_hot, model) not found.\")\n","    print(\"Please ensure data loading (cell 611e4c01) and model definition (cell aa8cb2ee) were run successfully.\")\n","    exit()\n","\n","# Re-define the model with a slightly adjusted architecture\n","# Let's add one more Dense layer and potentially adjust LSTM units if needed\n","# For now, keeping LSTM units the same but adding another Dense layer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import RMSprop # Using RMSprop optimizer\n","\n","# Get the number of features per frame and the maximum sequence length from the preprocessed data\n","# Ensure X_train, max_sequence_length, and num_classes are available from the previous data loading step (cell 611e4c01)\n","try:\n","    input_shape = (max_sequence_length, n_features) # Use variables from cell 611e4c01\n","    output_units = num_classes # Use variable from cell 611e4c01\n","    print(f\"Input shape for the model: {input_shape}\")\n","    print(f\"Output units (number of classes): {output_units}\")\n","\n","except NameError:\n","    print(\"Error: Required variables (max_sequence_length, n_features, num_classes) from data loading step (cell 611e4c01) not found.\")\n","    print(\"Please ensure cell 611e4c01 was run successfully before defining the model.\")\n","    # Exit or handle the error appropriately if variables are missing\n","    exit()\n","\n","\n","model = Sequential()\n","model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n","model.add(Dropout(0.2))\n","model.add(LSTM(128, return_sequences=True, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(LSTM(64, return_sequences=False, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(32, activation='relu')) # Added another Dense layer\n","model.add(Dense(output_units, activation='softmax')) # Output layer with softmax for classification\n","\n","print(\"Model architecture adjusted and defined.\")\n","\n","# Compile the model with RMSprop and a reduced learning rate\n","model.compile(optimizer=RMSprop(learning_rate=0.0005), # Using RMSprop with a slightly adjusted learning rate\n","              loss='categorical_crossentropy', # Use categorical_crossentropy for multi-class classification\n","              metrics=['categorical_accuracy']) # Use categorical_accuracy for evaluation\n","\n","print(\"Model compiled with RMSprop.\")\n","\n","# Define Early Stopping callback to prevent overfitting\n","# Monitor validation loss and stop training if it doesn't improve for a few epochs\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","print(\"Early Stopping callback defined.\")\n","\n","# Train the model\n","# Use the preprocessed training and testing data\n","history = model.fit(X_train, y_train_one_hot,\n","                    validation_data=(X_test, y_test_one_hot),\n","                    epochs=100, # Set a reasonable number of epochs, Early Stopping will prevent training for too long\n","                    callbacks=[early_stopping])\n","\n","print(\"\\nModel training complete.\")\n","\n","# Evaluate the model on the test data\n","loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# The trained model is now stored in the 'model' variable.\n","# The training history is in the 'history' variable."],"execution_count":18,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Compiling and training the model with RMSprop optimizer and adjusted architecture...\n","Input shape for the model: (194, 225)\n","Output units (number of classes): 2000\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Model architecture adjusted and defined.\n","Model compiled with RMSprop.\n","Early Stopping callback defined.\n","Epoch 1/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 792ms/step - categorical_accuracy: 0.0030 - loss: 7.5955 - val_categorical_accuracy: 0.0020 - val_loss: 7.5724\n","Epoch 2/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 769ms/step - categorical_accuracy: 0.0015 - loss: 10.5300 - val_categorical_accuracy: 9.8328e-04 - val_loss: 7.2980\n","Epoch 3/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 734ms/step - categorical_accuracy: 0.0023 - loss: 7.1045 - val_categorical_accuracy: 0.0020 - val_loss: 6.9608\n","Epoch 4/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 669ms/step - categorical_accuracy: 0.0039 - loss: 6.7391 - val_categorical_accuracy: 0.0020 - val_loss: 6.9508\n","Epoch 5/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 696ms/step - categorical_accuracy: 0.0021 - loss: 6.7035 - val_categorical_accuracy: 0.0020 - val_loss: 6.9340\n","Epoch 6/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 697ms/step - categorical_accuracy: 0.0038 - loss: 6.6754 - val_categorical_accuracy: 0.0020 - val_loss: 6.9290\n","Epoch 7/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 703ms/step - categorical_accuracy: 0.0021 - loss: 6.6725 - val_categorical_accuracy: 0.0020 - val_loss: 7.0626\n","Epoch 8/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 686ms/step - categorical_accuracy: 0.0050 - loss: 43357294592.0000 - val_categorical_accuracy: 9.8328e-04 - val_loss: 356949280.0000\n","Epoch 9/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 682ms/step - categorical_accuracy: 0.0033 - loss: 725071424.0000 - val_categorical_accuracy: 0.0020 - val_loss: 2017429760.0000\n","Epoch 10/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 693ms/step - categorical_accuracy: 0.0014 - loss: 1551506688.0000 - val_categorical_accuracy: 0.0000e+00 - val_loss: 1010147328.0000\n","Epoch 11/100\n","\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 683ms/step - categorical_accuracy: 0.0013 - loss: 505114560.0000 - val_categorical_accuracy: 9.8328e-04 - val_loss: 565950912.0000\n","\n","Model training complete.\n","Test Loss: 6.9290\n","Test Accuracy: 0.0020\n"]}]},{"cell_type":"markdown","metadata":{"id":"f82f912e"},"source":["## Convert model to tflite\n","\n","### Subtask:\n","Convert the trained model into a TensorFlow Lite format, which is optimized for on-device inference."]},{"cell_type":"markdown","metadata":{"id":"d2fcef28"},"source":["**Reasoning**:\n","Convert the trained Keras model to TensorFlow Lite format for on-device inference."]},{"cell_type":"code","metadata":{"id":"c2ede068","executionInfo":{"status":"ok","timestamp":1756396274416,"user_tz":-330,"elapsed":2380,"user":{"displayName":"Bala Kumaran","userId":"01169282059865830559"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"55585257-de42-48fc-e3f8-f8f7bd1fae78"},"source":["import tensorflow as tf\n","import os\n","\n","print(\"Converting the trained model to TensorFlow Lite format...\")\n","\n","# Ensure the 'model' variable is available from the model training step (cell f889c2de)\n","try:\n","    _ = model\n","except NameError:\n","    print(\"Error: Trained model variable 'model' not found.\")\n","    print(\"Please ensure the model training cell (f889c2de) was run successfully.\")\n","    exit()\n","\n","# Define the path to save the TFLite model\n","# Saving to Google Drive for persistence\n","# Ensure GOOGLE_DRIVE_PROJECT_ROOT is defined and Drive is mounted (run cell ed190a61 first)\n","try:\n","    _ = GOOGLE_DRIVE_PROJECT_ROOT\n","except NameError:\n","    print(\"Error: GOOGLE_DRIVE_PROJECT_ROOT is not defined.\")\n","    print(\"Please ensure the Google Drive mounting cell (ed190a61) was run successfully.\")\n","    exit()\n","\n","\n","TFLITE_MODELS_DIR_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'tflite_models')\n","os.makedirs(TFLITE_MODELS_DIR_DRIVE, exist_ok=True)\n","\n","tflite_model_path = os.path.join(TFLITE_MODELS_DIR_DRIVE, 'sign_language_model.tflite')\n","\n","# Create a TFLite converter\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","\n","# Apply optimizations for model size and performance (Optional)\n","# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","\n","# --- Apply the suggested fix for the ConverterError (from previous attempts) ---\n","# Allow the converter to use TensorFlow operations that don't have corresponding TFLite implementations\n","converter.target_spec.supported_ops = [\n","    tf.lite.OpsSet.TFLITE_BUILTINS,  # Standard TFLite operations\n","    tf.lite.OpsSet.SELECT_TF_OPS     # Allows using selected TensorFlow ops\n","]\n","# Disable the experimental lowering of TensorList operations\n","converter._experimental_lower_tensor_list_ops = False\n","# ----------------------------------------------------\n","\n","\n","# Convert the model\n","tflite_model = converter.convert()\n","\n","# Save the TFLite model to a file\n","with open(tflite_model_path, 'wb') as f:\n","    f.write(tflite_model)\n","\n","print(\"\\nModel successfully converted to TFLite format.\")\n","print(f\"TFLite model saved to: {tflite_model_path}\")\n","\n","# You can also check the size of the TFLite model\n","tflite_model_size = os.path.getsize(tflite_model_path)\n","print(f\"TFLite model size: {tflite_model_size / 1024:.2f} KB\")\n","\n","# The TFLite model file is now ready for integration into an Android application."],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Converting the trained model to TensorFlow Lite format...\n","Saved artifact at '/tmp/tmp2zdn_wkk'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 194, 225), dtype=tf.float32, name='keras_tensor')\n","Output Type:\n","  TensorSpec(shape=(None, 2000), dtype=tf.float32, name=None)\n","Captures:\n","  134532915616016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915616784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915615440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915616592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915616976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915619088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915620240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915619856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915621776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915620432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915620624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915622160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134532915621008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","\n","Model successfully converted to TFLite format.\n","TFLite model saved to: /content/drive/My Drive/my_sign_language_project/tflite_models/sign_language_model.tflite\n","TFLite model size: 1226.75 KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"94e74921"},"source":["## Test TFLite Model (in Colab)\n","\n","### Subtask:\n","Load the saved TFLite model and perform inference on a sample data point to verify its functionality before Android integration."]},{"cell_type":"markdown","metadata":{"id":"8abd5470"},"source":["**Reasoning**:\n","Load the TFLite model using the TensorFlow Lite Interpreter and perform inference on a sample from the test set (`X_test`) to check its output."]},{"cell_type":"code","metadata":{"id":"81c33a6e"},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","print(\"Loading the TFLite model and performing a test inference...\")\n","\n","# Ensure the TFLite model path is correctly defined (from cell c2ede068)\n","# Ensure X_test and actions (list of sign names) are available from data loading (cell 611e4c01)\n","try:\n","    GOOGLE_DRIVE_PROJECT_ROOT = '/content/drive/My Drive/my_sign_language_project' # Ensure this is defined\n","    TFLITE_MODELS_DIR_DRIVE = os.path.join(GOOGLE_DRIVE_PROJECT_ROOT, 'tflite_models')\n","    tflite_model_path = os.path.join(TFLITE_MODELS_DIR_DRIVE, 'sign_language_model.tflite')\n","\n","    _ = X_test\n","    _ = actions # The list of action names\n","except NameError:\n","    print(\"Error: Required variables (GOOGLE_DRIVE_PROJECT_ROOT, X_test, actions) not found.\")\n","    print(\"Please ensure Google Drive was mounted (cell ed190a61), data loading (cell 611e4c01), and TFLite conversion (cell c2ede068) were run successfully.\")\n","    exit()\n","\n","# Load the TFLite model\n","interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n","interpreter.allocate_tensors()\n","\n","# Get input and output tensors\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","print(f\"Input details: {input_details}\")\n","print(f\"Output details: {output_details}\")\n","\n","\n","# Select a sample from the test set for inference\n","# Let's use the first sample in the test set\n","sample_index = 0\n","input_data = X_test[sample_index]\n","input_label = y_test[sample_index] # Get the true numerical label\n","true_action = actions[input_label] # Get the true action name\n","\n","print(f\"\\nUsing sample {sample_index} from the test set.\")\n","print(f\"True action for this sample: {true_action}\")\n","print(f\"Input data shape: {input_data.shape}\")\n","print(f\"Input data dtype: {input_data.dtype}\")\n","\n","\n","# TFLite models typically expect float32 input\n","input_data = input_data.astype(input_details[0]['dtype'])\n","\n","# Add a batch dimension (TFLite models expect input in the shape of [batch_size, sequence_length, num_features])\n","input_data = np.expand_dims(input_data, axis=0)\n","\n","print(f\"Input data shape after adding batch dimension: {input_data.shape}\")\n","\n","\n","# Set the tensor to the input tensor\n","interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","# Run inference\n","print(\"\\nRunning inference...\")\n","interpreter.invoke()\n","print(\"Inference complete.\")\n","\n","# Get the output tensor\n","output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","print(f\"Output data shape: {output_data.shape}\")\n","print(f\"Output data: {output_data}\") # This will be the raw output (logits or probabilities)\n","\n","# Post-processing: Get the predicted class\n","predicted_label = np.argmax(output_data)\n","predicted_probability = np.max(output_data) # Get the confidence\n","\n","# Map the predicted label back to the action name\n","predicted_action = actions[predicted_label]\n","\n","print(f\"\\nPredicted label (numerical): {predicted_label}\")\n","print(f\"Predicted action: {predicted_action}\")\n","print(f\"Predicted probability: {predicted_probability:.4f}\")\n","\n","# Compare with the true label\n","print(f\"\\nTrue label (numerical): {input_label}\")\n","print(f\"True action: {true_action}\")\n","\n","if predicted_label == input_label:\n","    print(\"\\nPrediction is CORRECT!\")\n","else:\n","    print(\"\\nPrediction is INCORRECT.\")\n","\n","print(\"\\nTFLite model testing complete.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5948a58b"},"source":["## Generate Action List for Android\n","\n","### Subtask:\n","Generate the list of action names in a format suitable for copying into Android (Kotlin) code to map model output indices to human-readable sign words."]},{"cell_type":"markdown","metadata":{"id":"9af59329"},"source":["**Reasoning**:\n","Convert the list of action names (`actions`) into a Kotlin `List<String>` format, which can be easily copied and used in the Android application to map the model's numerical predictions back to the corresponding sign language words."]},{"cell_type":"code","metadata":{"id":"931355fd"},"source":["# ==============================================================================\n","# Convert the actions list to Kotlin List<String> format\n","# ==============================================================================\n","print(\"Generating Kotlin List<String> for actions...\")\n","\n","# Ensure the 'actions' variable is available from the data loading step (cell 611e4c01)\n","try:\n","    _ = actions\n","except NameError:\n","    print(\"Error: 'actions' variable not found.\")\n","    print(\"Please ensure the data loading and preprocessing cell (611e4c01) was run successfully.\")\n","    exit()\n","\n","# Format the Python list as a Kotlin List<String> string\n","kotlin_list_string = \"val actions = listOf<String>(\\n\"\n","for i, action in enumerate(actions):\n","    # Escape any double quotes within the action name\n","    escaped_action = action.replace('\"', '\\\\\"')\n","    kotlin_list_string += f'    \"{escaped_action}\"'\n","    if i < len(actions) - 1:\n","        kotlin_list_string += \",\"\n","    kotlin_list_string += \"\\n\"\n","kotlin_list_string += \")\"\n","\n","print(\"\\nCopy the following to your Kotlin code:\")\n","print(\"```kotlin\")\n","print(kotlin_list_string)\n","print(\"```\")\n","\n","print(\"\\nKotlin list generation complete.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1f4c327f"},"source":["## Integrate into android (conceptual)\n","\n","### Subtask:\n","Provide guidance on how to integrate the TFLite model into an Android application for real-time sign language recognition using MediaPipe for live keypoint extraction. (Note: This step will be conceptual as direct Android development in Colab is not possible)."]},{"cell_type":"markdown","metadata":{"id":"9202b977"},"source":["To integrate the TFLite model into an Android application for real-time sign language recognition, you'll generally follow these steps:\n","\n","1. **Set up Android Studio:** Make sure you have Android Studio installed and set up for development.\n","2. **Create a New Android Project:** Start a new Android project or open an existing one where you want to implement the sign language recognition.\n","3. **Add TensorFlow Lite and MediaPipe Dependencies:** Include the necessary libraries in your project's `build.gradle` file (usually the app-level `build.gradle`). You'll need the TFLite runtime and MediaPipe Tasks Vision library."]},{"cell_type":"markdown","metadata":{"id":"1a263082"},"source":["4. **Copy the TFLite Model:** Copy the generated `sign_language_model.tflite` file from your Google Drive (`/content/drive/My Drive/my_sign_language_project/tflite_models/sign_language_model.tflite`) into your Android project's `assets` folder. If you don't have an `assets` folder, you can create one inside your `main` source set (`app/src/main/assets`).\n","5. **Load the TFLite Model:** In your Android code (e.g., in an Activity or Fragment), load the TFLite model from the `assets` folder using a `Interpreter` object from the TensorFlow Lite library."]},{"cell_type":"markdown","metadata":{"id":"95f518c3"},"source":["6. **Set up MediaPipe for Real-time Keypoint Extraction:** Use MediaPipe Tasks Vision (specifically, the Pose, Face, and Hand landmark tasks, similar to what you used in Colab) to process the live camera feed from the Android device. MediaPipe will provide the real-time keypoint data.\n","   * You'll need to configure the MediaPipe tasks to run in \"live stream\" mode.\n","   * Set up listeners to receive the landmark results as frames are processed."]},{"cell_type":"markdown","metadata":{"id":"cef236f9"},"source":["7. **Feed Keypoints to the TFLite Model:** In the MediaPipe results listener, you will receive the keypoints for each frame.\n","   * You'll need to process these keypoints to match the input format of your TFLite model (the padded sequence of keypoints). This is a crucial step, as your TFLite model expects a sequence of keypoints, not just a single frame's keypoints.\n","   * You might need to maintain a buffer of recent keypoint frames and feed a sequence of these frames to the TFLite model when you have enough frames to form a complete sequence (matching the `max_sequence_length` your model was trained on).\n","   * Prepare the input tensor for the TFLite model using the processed keypoint sequence."]},{"cell_type":"markdown","metadata":{"id":"7bcd9073"},"source":["8. **Process Model Output:** The output from the TFLite model will be a probability distribution over the possible signs.\n","   * Find the class (sign) with the highest probability.\n","   * Map the predicted numerical label back to the sign language word using the action list you generated in Colab. You'll need to copy that Kotlin `List<String>` into your Android project.\n","9. **Display Results:** Display the recognized sign language word on the Android application's UI.\n","\n","This is a high-level overview. Implementing this will require writing Android code (Java or Kotlin), handling camera permissions, managing the MediaPipe processing pipeline, buffering keypoints, running TFLite inference, and updating the UI.\n","\n","The main steps of the plan have been covered. If you need further assistance with any specific part of this process or conceptual guidance on Android integration, feel free to ask!"]},{"cell_type":"markdown","metadata":{"id":"d7817ba6"},"source":["## Finish task\n","The task is complete as all the necessary code cells have been generated to perform the sign language recognition workflow, from data download and processing to model training, TFLite conversion and testing, and providing conceptual Android integration guidance."]},{"cell_type":"markdown","metadata":{"id":"3c7555ef"},"source":["# Task\n","Implement and train a Transformer-based model for sign language recognition using the provided keypoint data, evaluate its performance, and optionally convert it to TFLite format for potential Android integration."]},{"cell_type":"markdown","metadata":{"id":"e886266b"},"source":["## Implement transformer layers\n","\n","### Subtask:\n","Define the building blocks of the Transformer model, including multi-head attention, feed-forward networks, and positional encoding.\n"]},{"cell_type":"markdown","metadata":{"id":"3391e8a9"},"source":["**Reasoning**:\n","Define the necessary custom layers for the Transformer model, including Multi-Head Self-Attention, a Transformer Block combining attention and feed-forward, and Positional Encoding. This directly addresses all steps of the current subtask.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"80b50ce4"},"source":["## Define transformer model architecture\n","\n","### Subtask:\n","Assemble the Transformer layers into a complete model for sequence classification, taking into account the input shape and number of classes.\n"]},{"cell_type":"markdown","metadata":{"id":"f8bc4090"},"source":["**Reasoning**:\n","Assemble the Transformer layers into a complete model for sequence classification, taking into account the input shape and number of classes.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e52ec91b"},"source":["**Reasoning**:\n","The previous code failed because the `tf.range` function call within the `_calculate_positional_encodings` method of the `PositionalEncoding` layer is missing the required positional argument `limit` when being called. I need to fix the `PositionalEncoding` layer definition to correctly use `tf.range`. I will regenerate the full code block for assembling the model, including the corrected `PositionalEncoding` layer definition, to resolve this error and continue with the subtask.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2275cf49"},"source":["**Reasoning**:\n","The traceback indicates an `InvalidArgumentError` during the `tf.stack` operation within the `_calculate_positional_encodings` method of the `PositionalEncoding` layer. The shapes of the tensors being stacked (`tf.range(self.max_len)` and `tf.tile(tf.range(0, self.embed_dim, 2), [self.max_len])`) do not match. The issue is that `tf.tile` is tiling the range by `self.max_len`, resulting in a much larger tensor than `tf.range(self.max_len)`. I need to correct the indexing logic within `_calculate_positional_encodings` to correctly generate the indices for `tf.tensor_scatter_nd_update`. I will regenerate the full code block for assembling the model, including the corrected `PositionalEncoding` layer definition, to resolve this error and complete the subtask.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4a04b43b"},"source":["**Reasoning**:\n","The traceback indicates a `TypeError` because the `TransformerBlock` layer's `call` method is missing the required `training` argument when it is invoked within the model definition. The `training` argument is typically a boolean tensor indicating whether the model is in training mode or inference mode, and it's required by internal Keras layers like `Dropout` and `MultiHeadAttention`. When building a functional API model, this argument needs to be passed through the layers explicitly. I need to modify the model definition to pass the `training` argument to the `TransformerBlock` calls. I will regenerate the full code block for assembling the model, including passing the `training` argument.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7af01510"},"source":["## Compile the transformer model\n","\n","### Subtask:\n","Compile the defined Transformer model with a suitable optimizer and loss function.\n"]},{"cell_type":"markdown","metadata":{"id":"7c5a4604"},"source":["**Reasoning**:\n","Compile the defined Transformer model using the specified optimizer, loss function, and metrics, preparing it for the training process.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ab792db1"},"source":["## Train the transformer model\n","\n","### Subtask:\n","Train the compiled Transformer model using the prepared training and validation data.\n"]},{"cell_type":"markdown","metadata":{"id":"ad7a2085"},"source":["**Reasoning**:\n","Train the compiled Transformer model using the prepared training and validation data, incorporating an Early Stopping callback to monitor validation loss and prevent overfitting. This directly addresses the remaining steps of the subtask.\n","\n"]}]}